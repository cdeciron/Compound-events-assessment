{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c902a3cb-9050-4dad-8446-596db366a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pooch\n",
    "import cdsapi\n",
    "import os\n",
    "import xarray as xr\n",
    "import json\n",
    "import urllib\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import glob\n",
    "import cftime\n",
    "import seaborn as sns\n",
    "from shapely.ops import unary_union\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import List, Union, Optional\n",
    "from rasterio.transform import from_bounds\n",
    "import scipy.stats as st\n",
    "from scipy.stats import bootstrap\n",
    "from scipy.stats import kurtosis\n",
    "#Randomization of the data\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34534da9-f5d6-482d-9ad0-5010c9ff2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imput variables to find the path to your files\n",
    "location = 'Volumes'\n",
    "disk = 'LaCie 1'\n",
    "folder = 'Compound_events_study_folder'\n",
    "subfolder = 'Climate_models_data'\n",
    "gcm_rcm_folder = 'CNRM_CERFACS_CNRM_CM5_CNRM_ALADIN63' #depending on the gcm-rcm combination you are using here \n",
    "subfolder_2 = 'Post_processed_data'\n",
    "\n",
    "input_path = f'/{location}/{disk}/{folder}/{subfolder}/{gcm_rcm_folder}/{subfolder_2}'\n",
    "output_path = f'/{location}/{disk}/{folder}/{subfolder}/{gcm_rcm_folder}/Figures/Dark_doldrums'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36f1f3a-6489-4dbc-8ffb-fc00b9a6667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionnaries to find the csv files for all periods and for both variables\n",
    "period_names = {\n",
    "    'hist': 'historical', \n",
    "    'mid': 'mid-century', \n",
    "    'end': 'end-century'\n",
    "}\n",
    "\n",
    "variable_list = {\n",
    "    'rsds': 'Solar',\n",
    "    'sfcWind': 'Wind'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3603d705-8cff-4178-b669-44311c2650a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to convert 3hr solar data into daily data 'L' or 'D' (light or dark day) \n",
    "def label_dark_light_days(solar_series, solar_thresh=200):\n",
    "    solar_series = pd.to_numeric(solar_series, errors='coerce')\n",
    "    solar_series.index = pd.to_datetime(solar_series.index)\n",
    "\n",
    "    daily_labels = {}\n",
    "    daily_grouped = solar_series.groupby(solar_series.index.date)\n",
    "\n",
    "    for day, group in daily_grouped:\n",
    "        if (group > solar_thresh).sum() <= 2:\n",
    "            daily_labels[pd.to_datetime(day)] = 'D'\n",
    "        else:\n",
    "            daily_labels[pd.to_datetime(day)] = 'L'\n",
    "\n",
    "    return pd.Series(daily_labels, name=\"day_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb91aa8-9bb0-495d-85b6-e2fb4fb3c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the dark spell detection function \n",
    "def dark_spell(df, solar_threshold=200, count_threshold=5):\n",
    "    zones = ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
    "    dark_spell_count = {}\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure index is datetime and normalized (midnight)\n",
    "    df.index = pd.to_datetime(df.index).normalize()\n",
    "    \n",
    "    for zone in zones:\n",
    "        solar_col = f\"{zone}_solar\"\n",
    "        if solar_col not in df.columns:\n",
    "            raise ValueError(f\"Missing column: {solar_col}\")\n",
    "        \n",
    "        # Boolean: True where 'D'\n",
    "        dark_flag = (df[solar_col] == 'D')\n",
    "        dark_spell_flags = pd.Series(False, index=df.index)\n",
    "        \n",
    "        # Identify runs\n",
    "        run_ids = (dark_flag != dark_flag.shift()).cumsum()\n",
    "        groups = dark_flag.groupby(run_ids)\n",
    "        \n",
    "        # Count spells\n",
    "        spell_count = 0\n",
    "        for _, group_vals in groups:\n",
    "            if group_vals.iloc[0] and len(group_vals) >= count_threshold:\n",
    "                spell_count += 1\n",
    "                dark_spell_flags.loc[group_vals.index] = True\n",
    "        \n",
    "        dark_spell_count[zone] = spell_count\n",
    "        df[f'{zone}_dark_spell'] = dark_spell_flags\n",
    "    \n",
    "    return df, dark_spell_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc22b50b-e692-48f0-8ed0-8149e46b8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the low wind spells detection function\n",
    "def low_wind_spell(df, wind_thresh = 4, count_threshold = 5):\n",
    "    zones = ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
    "    low_wind_spell_count = {}\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure index is datetime and normalized (midnight)\n",
    "    df.index = pd.to_datetime(df.index).normalize()\n",
    "\n",
    "    for zone in zones:\n",
    "        wind_col = f\"{zone}_wind\"\n",
    "        if wind_col not in df.columns:\n",
    "            raise ValueError(f\"Missing column: {wind_col}\")\n",
    "\n",
    "        # Boolean: True if wind < threshold\n",
    "        low_wind_flag = (df[wind_col] < wind_thresh)\n",
    "        low_wind_spell_flags = pd.Series(False, index=df.index)\n",
    "\n",
    "        # Identify consecutive runs\n",
    "        wind_runs = (low_wind_flag != low_wind_flag.shift()).cumsum()\n",
    "        wind_groups = low_wind_flag.groupby(wind_runs)\n",
    "\n",
    "        # Count spells and mark flags\n",
    "        spell_count = 0\n",
    "        for _, group_vals in wind_groups:\n",
    "            if group_vals.iloc[0] and len(group_vals) >= count_threshold:\n",
    "                spell_count += 1\n",
    "                low_wind_spell_flags.loc[group_vals.index] = True\n",
    "\n",
    "        low_wind_spell_count[zone] = spell_count\n",
    "        df[f'{zone}_low_wind_spell'] = low_wind_spell_flags\n",
    "        \n",
    "    return df, low_wind_spell_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78201893-d672-474c-97c2-0e10f7054cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 10m height wind to 100m height wind according to the power law \n",
    "def power_law(wind_df):\n",
    "    z = 100\n",
    "    zref = 10\n",
    "    alpha = 0.143\n",
    "\n",
    "    dataset = wind_df.copy()\n",
    "    if isinstance(dataset.index, pd.DatetimeIndex):\n",
    "        dataset = dataset.sort_index()\n",
    "\n",
    "    df_wind_100m = pd.DataFrame(index=dataset.index)\n",
    "    for zone in ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']:\n",
    "        df_wind_100m[zone] = dataset[f'{zone}'] * (z / zref) ** alpha\n",
    "\n",
    "    return df_wind_100m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99a97ea-6890-4629-aa63-a4248f119fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pre-processing the csv files: merge wind and solar data on daily basis and keeping data only for winter months\n",
    "merged_daily_dict = {}  # Final output per period\n",
    "\n",
    "for period_key, period_code in period_names.items():\n",
    "    # --- Load solar (3-hourly) ---\n",
    "    solar_path = f\"{input_path}/rsds/regional_mean_rsds_{period_key}.csv\"\n",
    "    solar_df = pd.read_csv(solar_path, index_col=0, parse_dates=True)\n",
    "    solar_df = solar_df.rename(columns={zone: f\"{zone}_solar\" for zone in solar_df.columns})\n",
    "\n",
    "    # --- Label each day 'L' or 'D' for each zone ---\n",
    "    solar_labels = {}\n",
    "    for zone in solar_df.columns:\n",
    "        solar_labels[zone] = label_dark_light_days(solar_df[zone], solar_thresh=200)\n",
    "\n",
    "    solar_daily_labels = pd.DataFrame(solar_labels)\n",
    "\n",
    "    # Ensure common datetime index with wind df(daily at 00:00)\n",
    "    solar_daily_labels.index = pd.to_datetime(solar_daily_labels.index).normalize()\n",
    "\n",
    "    # --- Load wind (daily) ---\n",
    "    wind_path = f\"{input_path}/sfcWind/regional_mean_sfcWind_{period_key}.csv\"\n",
    "    wind_df = pd.read_csv(wind_path, index_col=0, parse_dates=True)\n",
    "    wind_df.index = wind_df.index.normalize()\n",
    "    df_wind_100m = power_law(wind_df)  # apply the power law to the current wind_df\n",
    "    df_wind_100m = df_wind_100m.rename(columns={zone: f\"{zone}_wind\" for zone in df_wind_100m.columns})\n",
    "    \n",
    "    # --- Merge on daily index ---\n",
    "    merged_df = solar_daily_labels.merge(df_wind_100m, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "    # --- Filter for winter months October to March ---\n",
    "    merged_df = merged_df[merged_df.index.month.isin([10, 11, 12, 1, 2, 3])]\n",
    "\n",
    "    merged_daily_dict[period_code] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96d9dbb-33b0-41b2-bb70-2edaabff2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the compound events function \n",
    "def compound_events(df, wind_thresh=4, min_spell_length=5):\n",
    "    zones = ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
    "\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index).normalize()\n",
    "\n",
    "    low_wind_spell_count = {}\n",
    "    dark_spell_count = {}\n",
    "    compound_event_count = {}\n",
    "    prob_ce = {}\n",
    "    zone_dark_doldrum_days = {}\n",
    "    compound_event_record = []\n",
    "\n",
    "    for zone in zones:\n",
    "        solar_col = f\"{zone}_solar\"\n",
    "        wind_col = f\"{zone}_wind\"\n",
    "\n",
    "        # --- Dark spells: runs of 5+ consecutive days where solar == 'D' ---\n",
    "        dark_flag = (df[solar_col] == 'D')\n",
    "        dark_spell_flags = pd.Series(False, index=df.index)\n",
    "\n",
    "        dark_runs = (dark_flag != dark_flag.shift()).cumsum()\n",
    "        dark_groups = dark_flag.groupby(dark_runs)\n",
    "\n",
    "        dark_spell_count_zone = 0\n",
    "        for group_id, group_vals in dark_groups:\n",
    "            if group_vals.iloc[0] == True and len(group_vals) >= min_spell_length:\n",
    "                dark_spell_count_zone += 1\n",
    "                dark_spell_flags.loc[group_vals.index] = True\n",
    "\n",
    "        df[f'{zone}_dark_spell'] = dark_spell_flags\n",
    "\n",
    "        # --- Low wind spells: runs of 5+ consecutive days where wind < wind_thresh ---\n",
    "        low_wind_flag = (df[wind_col] < wind_thresh)\n",
    "        low_wind_spell_flags = pd.Series(False, index=df.index)\n",
    "\n",
    "        wind_runs = (low_wind_flag != low_wind_flag.shift()).cumsum()\n",
    "        wind_groups = low_wind_flag.groupby(wind_runs)\n",
    "\n",
    "        low_wind_spell_count_zone = 0\n",
    "        for group_id, group_vals in wind_groups:\n",
    "            if group_vals.iloc[0] == True and len(group_vals) >= min_spell_length:\n",
    "                low_wind_spell_count_zone += 1\n",
    "                low_wind_spell_flags.loc[group_vals.index] = True\n",
    "\n",
    "        df[f'{zone}_low_wind_spell'] = low_wind_spell_flags\n",
    "\n",
    "        # --- Overlapping compound events ---\n",
    "        overlap = dark_spell_flags & low_wind_spell_flags\n",
    "        overlap_dates = df.index[overlap]\n",
    "\n",
    "        zone_dark_doldrum_days[zone] = overlap_dates\n",
    "\n",
    "        for dt in overlap_dates:\n",
    "            try:\n",
    "                idx = df.index.get_loc(dt)\n",
    "                compound_event_record.append({'zone': zone, 'date': dt, 'original_index': idx})\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        low_wind_spell_count[zone] = low_wind_spell_count_zone\n",
    "        dark_spell_count[zone] = dark_spell_count_zone\n",
    "        compound_event_count[zone] = len(overlap_dates)\n",
    "\n",
    "        if len(df) > 0:\n",
    "            prob_ce[zone] = round((len(overlap_dates) / len(df)) * 100, 3)\n",
    "        else:\n",
    "            prob_ce[zone] = 0\n",
    "\n",
    "    compound_event_index_dict = {\n",
    "        zone: [df.index.get_loc(dt) for dt in dates if dt in df.index]\n",
    "        for zone, dates in zone_dark_doldrum_days.items()\n",
    "    }\n",
    "\n",
    "    compound_event_index_df = pd.DataFrame({\n",
    "        zone: pd.Series(idxs) for zone, idxs in compound_event_index_dict.items()\n",
    "    })\n",
    "\n",
    "    compound_event_indexes = pd.DataFrame(compound_event_record)\n",
    "\n",
    "    print('Low wind spells:', low_wind_spell_count)\n",
    "    print('Dark spells:', dark_spell_count)\n",
    "    print('Compound events (CEs):', compound_event_count)\n",
    "    print('Probability of a CE (%):', prob_ce)\n",
    "\n",
    "    return (\n",
    "        low_wind_spell_count,\n",
    "        dark_spell_count,\n",
    "        prob_ce,\n",
    "        compound_event_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebaa813-c510-4853-9246-ce5116a0d621",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low wind spells: {'NO1': 271, 'NO2': 12, 'NO3': 8, 'NO4': 3, 'NO5': 201}\n",
      "Dark spells: {'NO1': 38, 'NO2': 42, 'NO3': 33, 'NO4': 37, 'NO5': 41}\n",
      "Compound events (CEs): {'NO1': 4162, 'NO2': 65, 'NO3': 42, 'NO4': 15, 'NO5': 1445}\n",
      "Probability of a CE (%): {'NO1': 76.116, 'NO2': 1.189, 'NO3': 0.768, 'NO4': 0.274, 'NO5': 26.426}\n",
      "Low wind spells: {'NO1': 287, 'NO2': 19, 'NO3': 31, 'NO4': 3, 'NO5': 211}\n",
      "Dark spells: {'NO1': 39, 'NO2': 40, 'NO3': 33, 'NO4': 33, 'NO5': 37}\n",
      "Compound events (CEs): {'NO1': 4092, 'NO2': 112, 'NO3': 166, 'NO4': 17, 'NO5': 1715}\n",
      "Probability of a CE (%): {'NO1': 74.835, 'NO2': 2.048, 'NO3': 3.036, 'NO4': 0.311, 'NO5': 31.364}\n",
      "Low wind spells: {'NO1': 288, 'NO2': 33, 'NO3': 28, 'NO4': 9, 'NO5': 213}\n",
      "Dark spells: {'NO1': 40, 'NO2': 41, 'NO3': 40, 'NO4': 36, 'NO5': 42}\n",
      "Compound events (CEs): {'NO1': 4125, 'NO2': 151, 'NO3': 160, 'NO4': 46, 'NO5': 1683}\n",
      "Probability of a CE (%): {'NO1': 75.453, 'NO2': 2.762, 'NO3': 2.927, 'NO4': 0.841, 'NO5': 30.785}\n"
     ]
    }
   ],
   "source": [
    "#Results for number of hazards and compound events for all periods\n",
    "low_wind_count_hist, dark_days_count_hist, prob_hist, ce_count_hist = compound_events(merged_daily_dict['historical'])\n",
    "low_wind_count_mid, dark_days_count_mid, prob_mid, ce_count_mid = compound_events(merged_daily_dict['mid-century'])\n",
    "low_wind_count_end, dark_days_count_end, prob_end, ce_count_end = compound_events(merged_daily_dict['end-century'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2948420d-503d-42b5-8089-8a2007abfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrapping experiment  \n",
    "def hypotheses_test(df, n_bootstrap=1000, wind_thresh=4, min_spell_length=5):\n",
    "    \"\"\"\n",
    "    Perform bootstrap hypothesis testing for compound events (dark + low wind spells).\n",
    "\n",
    "    Returns a DataFrame with bootstrapped counts:\n",
    "    - Rows: bootstrap iterations\n",
    "    - Columns: zones NO1-NO5\n",
    "    \"\"\"\n",
    "    zones = ['NO1', 'NO2', 'NO3', 'NO4', 'NO5']\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample rows with replacement (shuffle but keep index)\n",
    "        synthetic_df = df.sample(n=len(df), replace=True).reset_index(drop=True)\n",
    "\n",
    "        iteration_counts = {}\n",
    "        for zone in zones:\n",
    "            solar_series = synthetic_df[f'{zone}_solar']\n",
    "            wind_series = synthetic_df[f'{zone}_wind']\n",
    "\n",
    "            # --- Dark spells ---\n",
    "            dark_flag = solar_series\n",
    "            dark_runs = (dark_flag != dark_flag.shift()).cumsum()\n",
    "            dark_groups = dark_flag.groupby(dark_runs)\n",
    "            dark_spell_flags = pd.Series(False, index=synthetic_df.index)\n",
    "            for _, group_vals in dark_groups:\n",
    "                if group_vals.iloc[0] and len(group_vals) >= min_spell_length:\n",
    "                    dark_spell_flags.loc[group_vals.index] = True\n",
    "\n",
    "            # --- Low wind spells ---\n",
    "            low_wind_flag = (wind_series < wind_thresh)\n",
    "            wind_runs = (low_wind_flag != low_wind_flag.shift()).cumsum()\n",
    "            wind_groups = low_wind_flag.groupby(wind_runs)\n",
    "            low_wind_spell_flags = pd.Series(False, index=synthetic_df.index)\n",
    "            for _, group_vals in wind_groups:\n",
    "                if group_vals.iloc[0] and len(group_vals) >= min_spell_length:\n",
    "                    low_wind_spell_flags.loc[group_vals.index] = True\n",
    "\n",
    "            # --- Compound events ---\n",
    "            overlap = dark_spell_flags & low_wind_spell_flags\n",
    "            iteration_counts[zone] = overlap.sum()\n",
    "\n",
    "        results.append(iteration_counts)\n",
    "\n",
    "    # Convert list of dicts to DataFrame\n",
    "    bootstrapped_df = pd.DataFrame(results)\n",
    "    return bootstrapped_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5aaac3-20c4-4ccd-96ad-7159f25dde0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boostrapped_hist = hypotheses_test(merged_daily_dict['historical'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f53a1f1-7b67-44d6-b37d-7b07073cdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "boostrapped_mid = hypotheses_test(merged_daily_dict['mid-century'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd330bc0-9684-4bb1-b50d-8a5a95736200",
   "metadata": {},
   "outputs": [],
   "source": [
    "boostrapped_end = hypotheses_test(merged_daily_dict['end-century'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7d20985-de90-43cc-ad01-5797f69e3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "#Plotting frequency histograms \n",
    "def plot_hypothesis_test_histograms_freq(\n",
    "    bstp_hist, bstp_mid, bstp_end, og_ce_count_hist, og_ce_count_mid, og_ce_count_end, label, output_path, n_number\n",
    "):\n",
    "    # Convert each bootstrap set to DataFrame\n",
    "    hist_df = pd.DataFrame(bstp_hist)\n",
    "    mid_df  = pd.DataFrame(bstp_mid)\n",
    "    end_df  = pd.DataFrame(bstp_end)\n",
    "\n",
    "    # Iterate over regions\n",
    "    for region in hist_df.columns:\n",
    "        data_hist = hist_df[region]\n",
    "        data_mid  = mid_df[region]\n",
    "        data_end  = end_df[region]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "        # Plot all three periods with different colors\n",
    "        sns.histplot(data_hist, bins=15, kde=True, color=\"#4c72b0\", edgecolor='black', label=\"Historical\", ax=ax, alpha=0.5)\n",
    "        sns.histplot(data_mid,  bins=15, kde=True, color=\"#55a868\", edgecolor='black', label=\"Mid-century\", ax=ax, alpha=0.5)\n",
    "        sns.histplot(data_end,  bins=15, kde=True, color=\"#c44e52\", edgecolor='black', label=\"End-century\", ax=ax, alpha=0.5)\n",
    "\n",
    "        # Add vertical line for observed/original value\n",
    "        threshold_hist = og_ce_count_hist.get(region, None)\n",
    "        threshold_mid = og_ce_count_mid.get(region, None)\n",
    "        threshold_end = og_ce_count_end.get(region, None)\n",
    "        \n",
    "        ax.axvline(threshold_hist, color=\"#4c72b0\", linestyle=\"--\", linewidth=1.5)\n",
    "        ax.axvline(threshold_mid, color=\"#55a868\", linestyle=\"--\", linewidth=1.5)\n",
    "        ax.axvline(threshold_end, color=\"#c44e52\", linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_title(f'{region} Compound Events ({n_number})', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Number of Compound Events', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        ax.set_axisbelow(True)\n",
    "        sns.despine(trim=True)\n",
    "        ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "        # Save\n",
    "        save_path = f\"{output_path}/Bootstrap/{region}_CE_all_periods_{label}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cb113e4b-bd09-431f-ada8-889bf596c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hypothesis_test_histograms_freq(boostrapped_hist, boostrapped_mid, boostrapped_end, \n",
    "                                                ce_count_hist, ce_count_mid, ce_count_end,\n",
    "                                                'freq', output_path, 'n=1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4fc08cf9-d0b0-4993-8f58-41c64dc0b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "#Plotting density histograms\n",
    "def plot_hypothesis_test_histograms_density(\n",
    "    bstp_hist, bstp_mid, bstp_end, og_ce_count_hist, og_ce_count_mid, og_ce_count_end, label, output_path, n_number\n",
    "):\n",
    "    # Convert each bootstrap set to DataFrame\n",
    "    hist_df = pd.DataFrame(bstp_hist)\n",
    "    mid_df  = pd.DataFrame(bstp_mid)\n",
    "    end_df  = pd.DataFrame(bstp_end)\n",
    "\n",
    "    for region in hist_df.columns:\n",
    "        data_hist = hist_df[region].dropna()\n",
    "        data_mid  = mid_df[region].dropna()\n",
    "        data_end  = end_df[region].dropna()\n",
    "\n",
    "        # Define common bin edges for all three datasets\n",
    "        all_data = pd.concat([data_hist, data_mid, data_end])\n",
    "        bins = np.histogram_bin_edges(all_data, bins=15)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "        # Plot histograms with density scaling\n",
    "        sns.histplot(\n",
    "            data_hist, bins=bins, stat=\"density\", kde=True,\n",
    "            color=\"#4c72b0\", edgecolor='black', label=\"Historical\", ax=ax, alpha=0.5\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_mid, bins=bins, stat=\"density\", kde=True,\n",
    "            color=\"#55a868\", edgecolor='black', label=\"Mid-century\", ax=ax, alpha=0.5\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_end, bins=bins, stat=\"density\", kde=True,\n",
    "            color=\"#c44e52\", edgecolor='black', label=\"End-century\", ax=ax, alpha=0.5\n",
    "        )\n",
    "\n",
    "        # Add vertical line for observed/original value\n",
    "        threshold_hist = og_ce_count_hist.get(region, None)\n",
    "        threshold_mid = og_ce_count_mid.get(region, None)\n",
    "        threshold_end = og_ce_count_end.get(region, None)\n",
    "        \n",
    "        ax.axvline(threshold_hist, color=\"#4c72b0\", linestyle=\"--\", linewidth=1.5)\n",
    "        ax.axvline(threshold_mid, color=\"#55a868\", linestyle=\"--\", linewidth=1.5)\n",
    "        ax.axvline(threshold_end, color=\"#c44e52\", linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "        # Labels and styling\n",
    "        ax.set_title(f'{region} Compound Events ({n_number})', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Number of Compound Events', fontsize=12)\n",
    "        ax.set_ylabel('Density', fontsize=12)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        ax.set_axisbelow(True)\n",
    "        sns.despine(trim=True)\n",
    "        ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "        # Save file\n",
    "        save_path = f\"{output_path}/Bootstrap/{region}_CE_all_periods_{label}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08dd1634-7ffd-4194-8fe6-c77f12f2e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hypothesis_test_histograms_density(boostrapped_hist, boostrapped_mid, boostrapped_end, \n",
    "                                                ce_count_hist, ce_count_mid, ce_count_end,\n",
    "                                                'density', output_path, 'n=1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f744af-1886-4766-adfd-977e6a6837e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
